{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":1392716,"sourceType":"datasetVersion","datasetId":813202},{"sourceId":166623189,"sourceType":"kernelVersion"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary\n\nThe target of the project is to create a supervised model with sequencial data deep learning model, to predict if a tweet indicates a real disaster. This is useful for some organizations to actively monitoring tweets for deteting disaster in near real time. ","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\nimport random\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchmetrics\n\n# Import tqdm for progress bar\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:34:17.464275Z","iopub.execute_input":"2024-03-13T17:34:17.464840Z","iopub.status.idle":"2024-03-13T17:34:17.472449Z","shell.execute_reply.started":"2024-03-13T17:34:17.464793Z","shell.execute_reply":"2024-03-13T17:34:17.471100Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed()","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:34:18.480217Z","iopub.execute_input":"2024-03-13T17:34:18.480782Z","iopub.status.idle":"2024-03-13T17:34:18.490662Z","shell.execute_reply.started":"2024-03-13T17:34:18.480754Z","shell.execute_reply":"2024-03-13T17:34:18.489187Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"KFOLD = 5\n\nGLOVE_FILE = '/kaggle/input/glove6b/glove.6B.200d.txt'\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:34:19.096052Z","iopub.execute_input":"2024-03-13T17:34:19.096404Z","iopub.status.idle":"2024-03-13T17:34:19.101777Z","shell.execute_reply.started":"2024-03-13T17:34:19.096378Z","shell.execute_reply":"2024-03-13T17:34:19.100769Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"train_df=pickle.load(open(\"/kaggle/input/nlp-disastertweets-eda-cleaning/train_cleaned_df.pkl\", \"rb\"))\ntest_df=pickle.load(open(\"/kaggle/input/nlp-disastertweets-eda-cleaning/test_cleaned_df.pkl\", \"rb\"))","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-13T17:34:20.051794Z","iopub.execute_input":"2024-03-13T17:34:20.052162Z","iopub.status.idle":"2024-03-13T17:34:20.085892Z","shell.execute_reply.started":"2024-03-13T17:34:20.052139Z","shell.execute_reply":"2024-03-13T17:34:20.083769Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize the scentences\n\nHere I simply separate the words, since we have done cleaning","metadata":{}},{"cell_type":"code","source":"def tokenize(text):\n    words = text.lower().split() \n    # separate non letters from words, e.g. \"100year\" -> \"100\" \"year\"\n    result = []\n    for word in words:\n        result.extend(list(filter(None, re.split(r'([a-z]+)', word))))\n    return result\n\ntrain_df['text_token'] = train_df['text'].apply(lambda x : tokenize(x))\ntest_df['text_token'] = test_df['text'].apply(lambda x : tokenize(x))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:34:21.237373Z","iopub.execute_input":"2024-03-13T17:34:21.237691Z","iopub.status.idle":"2024-03-13T17:34:21.507012Z","shell.execute_reply.started":"2024-03-13T17:34:21.237667Z","shell.execute_reply":"2024-03-13T17:34:21.505677Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this earthquake Ma...   \n1   4     NaN      NaN              Forest fire near La Ronge Sask Canada   \n2   5     NaN      NaN  All residents asked to shelter in place are be...   \n3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n4   7     NaN      NaN  Just got sent this photo from Ruby Alaska as s...   \n\n   target                                         text_token  \n0       1  [our, deeds, are, the, reason, of, this, earth...  \n1       1      [forest, fire, near, la, ronge, sask, canada]  \n2       1  [all, residents, asked, to, shelter, in, place...  \n3       1  [13000, people, receive, wildfires, evacuation...  \n4       1  [just, got, sent, this, photo, from, ruby, ala...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text_token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n      <td>1</td>\n      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask Canada</td>\n      <td>1</td>\n      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to shelter in place are be...</td>\n      <td>1</td>\n      <td>[all, residents, asked, to, shelter, in, place...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13000 people receive wildfires evacuation orde...</td>\n      <td>1</td>\n      <td>[13000, people, receive, wildfires, evacuation...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby Alaska as s...</td>\n      <td>1</td>\n      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Word embedding\nBefore we can train a model, we need to embed the words in to dense vectors for the input of the recurance neuro network.\nHere we use GloVe embedding with pretrained data set from https://www.kaggle.com/datasets/yesornope/glove6b","metadata":{}},{"cell_type":"code","source":"%%time\n\n# create embedding index\nprint(\"Loading \", GLOVE_FILE)\n\nembedding_index = {}\nif GLOVE_FILE.endswith(\"pkl\"):\n    \n    embedding_index = np.load(GLOVE_FILE, allow_pickle=True)\nelse:\n    with open(GLOVE_FILE, encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embedding_index[word] = coefs","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:34:23.740413Z","iopub.execute_input":"2024-03-13T17:34:23.741063Z","iopub.status.idle":"2024-03-13T17:34:42.837967Z","shell.execute_reply.started":"2024-03-13T17:34:23.741034Z","shell.execute_reply":"2024-03-13T17:34:42.837099Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Loading  /kaggle/input/glove6b/glove.6B.200d.txt\nCPU times: user 14.2 s, sys: 893 ms, total: 15.1 s\nWall time: 19.1 s\n","output_type":"stream"}]},{"cell_type":"code","source":"# We will use the max_len as the input sequence length, text would be padded or truncated (possible in test data)\nmax_len = max(train_df['text_token'].apply(lambda x : len(x)))\nprint(\"max text token length:\", max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data_embedding(data_tokens, max_len=max_len, embedding_index=embedding_index):\n    print(\"Input data tokens shape:\", data_tokens.shape)\n    missing_embedding_count = 0\n    total_words_count = 0\n    missing_embedding_words = {}\n    embedding_dim = next(iter(embedding_index.values())).shape[0]\n    output = np.zeros((data_tokens.shape[0], max_len, embedding_dim))\n    print(\"Output shape:\", output.shape)\n    for i in range(len(data_tokens)):\n        scentence_tokens = data_tokens[i]\n        for j in range(min(max_len, len(scentence_tokens))):\n            word = scentence_tokens[j]\n            total_words_count += 1\n            if word in embedding_index:\n                embeding = embedding_index[word]\n                output[i, j, :] = embeding\n            else:\n                #print(\"missing embedding in i:\", i, word, \":\", scentence_tokens)\n                missing_embedding_words[word] = missing_embedding_words.get(word, 0) + 1\n                missing_embedding_count += 1\n                \n    print(f\"Embedding coverage: {round(100*(total_words_count-missing_embedding_count)/total_words_count, 1)}%\")\n    print(f\"Missing embedding unique words: {len(missing_embedding_words)}\")\n    #print(sorted(missing_embedding_words.items(), key=lambda x:x[1], reverse=True))\n    return output, missing_embedding_words\n\nX, missing_embedding_words_train = prepare_data_embedding(train_df['text_token'].values)\ny = train_df[\"target\"].values\n\nX_test, missing_embedding_words_test = prepare_data_embedding(test_df['text_token'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create train/val data","metadata":{}},{"cell_type":"code","source":"\n\nkfold = KFold(n_splits=KFOLD, random_state=42, shuffle=True)\n\nfolds_idx_train = []\nfolds_idx_val = []\n\nfor fold_idx, (train_index, val_index) in enumerate(kfold.split(X)):\n    print(\"fold\", fold_idx, \"train:\", len(train_index), \"val:\", len(val_index))\n    folds_idx_train.append(train_index)\n    folds_idx_val.append(val_index)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create torch Dataset\nDataset can be used by Dataloader for input of the model","metadata":{}},{"cell_type":"code","source":"class NLPDs(Dataset):\n    def __init__(self, X, y=None):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, index):\n        X_item = torch.tensor(self.X[index], dtype=torch.float)\n        y_item = torch.tensor(self.y[index], dtype=torch.float) if self.y is not None else torch.tensor([])\n        return {\n            'X': X_item,\n            'y': y_item\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model selection\nSince this is a excercise for RNN, I will compare both pure RNN and LSTM if the perform differently.\nAfter testing both RNN and LSTM, it turns out LSTM has a much better performance than RNN. TO keep the Notebook concise, I have't keep the RNN code, but just show the best Model.","metadata":{}},{"cell_type":"code","source":"embedding_dim = next(iter(embedding_index.values())).shape[0]\n\nclass RNNClassifier(nn.Module):\n\n    def __init__(self, \n                 embedding_dim=embedding_dim, \n                 rnn_hidden_dim=128,\n                 rnn_bidirectional=False,\n                 rnn_num_layers=1,\n                 fc_hidden_1_dim=128,\n                 fc_hidden_2_dim=128,\n                 vocab_size=max_len,\n                 dropout_p=0):\n        super(RNNClassifier, self).__init__()\n        self.dropout_p = dropout_p\n        self.rnn = nn.LSTM(embedding_dim, rnn_hidden_dim, bidirectional=rnn_bidirectional, num_layers=rnn_num_layers, batch_first=True)\n        \n        rnn_features = rnn_hidden_dim * rnn_num_layers\n        if rnn_bidirectional:\n            rnn_features = rnn_features * 2\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout_p),\n            nn.Linear(rnn_features, fc_hidden_1_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_p),\n            nn.Linear(fc_hidden_1_dim, fc_hidden_2_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_p),\n            nn.Linear(fc_hidden_2_dim, 1),\n\n        )\n    def forward(self, X):\n        _, (out, _) = self.rnn(X)\n\n        out = torch.transpose(out, 0, 1)\n        out = torch.flatten(out, start_dim=1)\n        out = self.classifier(out)\n        \n        return out\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"markdown","source":"## Hyper parameters tuning\n\nHyper parameters tuning is very important, it decide the model training speed and perforamnce.\nIn my model, I turned the following parameers: learning rate, learning rate scheduler, optimizer, model size (LSTM hidden layer size, fully connected hidden layer size, etc.), regulation parameters like dropout rate, weight_decay, etc.\n\nIn order to keep the notebook concise, I didn't keep the code and result for all the different values of parameters, but put the final optimized values instead.","metadata":{}},{"cell_type":"code","source":"import copy\nfrom IPython.display import display, clear_output\nfrom torchmetrics.classification import BinaryAccuracy, BinaryF1Score\n\n@torch.inference_mode()\ndef validate(model, val_loader):\n    \n    total_loss = 0\n    loss_function = nn.BCEWithLogitsLoss()\n    val_acc = BinaryAccuracy()\n    val_f1 = BinaryF1Score()\n    model.eval()\n    \n    for step, data in enumerate(val_loader):\n        \n        X, y = data[\"X\"], data[\"y\"]\n        \n        output = model(X)\n        output = torch.squeeze(output, dim=1)\n        loss = loss_function(output, y)\n        \n        output = F.sigmoid(output)\n        val_acc(output, y)    \n        val_f1(output, y)\n        \n        total_loss += loss.item()\n\n    return total_loss/(len(val_loader)), val_acc.compute().item(), val_f1.compute().item()\n\n\ndef train(model, train_loader, fold_idx, epochs, loss_function, optimizer, scheduler):\n    \n    \n    history = {\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"train_f1\": [],\n        \"val_loss\": [],\n        \"val_acc\": [],\n        \"val_f1\": []\n    }\n    best_val_f1 = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        \n        total_loss = 0\n        train_acc = BinaryAccuracy()\n        train_f1 = BinaryF1Score()\n\n        for step, data in enumerate(train_loader):\n\n            optimizer.zero_grad()\n            X, y = data[\"X\"], data[\"y\"]\n            \n            \n            output = torch.squeeze(model(X), dim=1)\n            loss = loss_function(output, y)\n            \n            output = F.sigmoid(output)\n            train_acc(output, y)\n            temp_f1 = train_f1(output, y)\n            \n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            \n            current_train_loss = (total_loss/(step+1))\n            current_train_acc = train_acc.compute().item()\n            current_train_f1 = train_f1.compute().item()\n            current_lr = round(scheduler.get_last_lr()[0], 5)\n            \n        scheduler.step()\n        \n        val_loss, val_acc, val_f1 = validate(model, val_loader)\n        \n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            \n            best_val_model_wts = copy.deepcopy(model.state_dict())\n            torch.save(best_val_model_wts, f\"fold_{fold_idx}_best_model.bin\")\n        \n        print(f\"Fold {fold_idx} Epoch {epoch}: \"\n              f\"loss={round(current_train_loss, 5)}, \"\n              f\"val_loss={round(val_loss, 5)}, \"\n              f\"acc={round(current_train_acc, 5)}, \"\n              f\"val_acc={round(val_acc, 5)}, \"\n              f\"f1={round(current_train_f1, 5)}, \"\n              f\"val_f1={round(val_f1, 5)}, \"\n              f\"lr={round(current_lr, 5)}\"\n             )\n        history[\"train_loss\"].append(current_train_loss)\n        history[\"train_acc\"].append(current_train_acc)\n        history[\"train_f1\"].append(current_train_f1)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        history[\"val_f1\"].append(val_f1)\n    \n    print(\"Fold\", fold_idx, \"best val F1 score:\", best_val_f1)\n    return best_val_f1, history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! rm -rf /kaggle/working/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\nLR = 1e-2\nMIN_LR = 1e-4\n\ntotal_f1 = 0\nfor fold_idx in range(len(folds_idx_train)):\n    train_idx = folds_idx_train[fold_idx]\n    val_idx = folds_idx_val[fold_idx]\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n    \n    train_ds = NLPDs(X_train, y_train)\n    val_ds = NLPDs(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=64, num_workers=2, shuffle=True, pin_memory=True, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=32, num_workers=2, shuffle=False, pin_memory=True, drop_last=False)\n\n    pos_weights = np.sum(1-y_train) / np.sum(y_train)\n    print(\"pos_weights:\", pos_weights)\n    \n    model = RNNClassifier(\n        rnn_bidirectional=True, \n        rnn_num_layers=2,\n        rnn_hidden_dim=32, \n        fc_hidden_1_dim=64, \n        fc_hidden_2_dim=64, \n        dropout_p=0.8)\n    \n    loss_function = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weights))\n\n    optimizer = optim.Adam(model.parameters(), lr=LR, betas = (0.9,0.999),eps = 1.0*1e-8, weight_decay=2e-5)\n    scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=EPOCHS, eta_min=MIN_LR)\n\n\n    best_val_f1, history = train(model, train_loader, fold_idx, EPOCHS, loss_function, optimizer, scheduler)\n    total_f1 += best_val_f1\n    \n    \nprint(\"Average val F1 score:\", total_f1/len(folds_idx_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the metrics of the last fold","metadata":{}},{"cell_type":"code","source":"def plot_train_history(history, metric=\"acc\"):\n    train_metric = history[f\"train_{metric}\"]\n    val_metric = history[f\"val_{metric}\"]\n    plt.plot(history[f\"train_{metric}\"])\n    plt.plot(history[f\"val_{metric}\"])\n    plt.title(f\"{metric}\")\n    plt.ylabel(metric)\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n\nplot_train_history(history, \"f1\")\nplot_train_history(history, \"acc\")\nplot_train_history(history, \"loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference on test data","metadata":{}},{"cell_type":"markdown","source":"## Load models\n\nEnsemble models from each fold and average the results.","metadata":{}},{"cell_type":"code","source":"def load_weights(model, weights_path):\n    if os.path.isfile(weights_path):\n        model.load_state_dict(torch.load(weights_path))\n        print(\"Loaded weights from\", weights_path)\n        return True\n    else:\n        print(\"No previous weights available at\", weights_path)\n        return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodels = []\nfor fold_idx in range(len(folds_idx_train)):\n    model = RNNClassifier(\n        rnn_bidirectional=True, \n        rnn_num_layers=2,\n        rnn_hidden_dim=32, \n        fc_hidden_1_dim=64, \n        fc_hidden_2_dim=64, \n    )\n    load_weights(model, f\"/kaggle/working/fold_{fold_idx}_best_model.bin\")\n    models.append(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = NLPDs(X_test)\ntest_loader = DataLoader(test_ds, batch_size=32, num_workers=2, shuffle=False, pin_memory=True, drop_last=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.inference_mode()\ndef infer_by_one_model(model, test_loader):\n    progress_bar = tqdm(total=len(test_loader), desc=f\"Test\", leave=True)\n    \n    model.eval()\n    pred_list = []\n    \n    for step, data in enumerate(test_loader):\n        X = data[\"X\"]\n        output = model(X)\n        output = torch.squeeze(output, dim=1)\n        pred_list.append(output.detach().cpu().numpy())\n        progress_bar.update()\n    \n    progress_bar.close()\n    return np.concatenate(pred_list)\n\nmodels_results = []\nfor model in models:\n    models_results.append(infer_by_one_model(model, test_loader))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prob = np.sum(np.stack(models_results), axis=0) / len(models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"target_prob\"] = prob\ntest_df[\"target\"] = test_df[\"target_prob\"].apply(lambda x: 1 if x>=0.5 else 0)\nsum(test_df[\"target\"]==1)\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smpl_sub = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nsub = pd.merge(smpl_sub[[\"id\"]], test_df[[\"id\",\"target\"]], on=\"id\", how=\"left\")\n\nsub.to_csv(\"submission.csv\", index=False)\n\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}