{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":166178304,"sourceType":"kernelVersion"}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary\n\nThe target of the project is to create a supervised model with sequencial data deep learning model, to predict if a tweet indicates a real disaster. This is useful for some organizations to actively monitoring tweets for deteting disaster in near real time. ","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport string\nimport pickle\nimport time\nimport gc\n\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchmetrics\n\n# Import tqdm for progress bar\nfrom tqdm.notebook import tqdm\n\nimport transformers\nfrom transformers import BertTokenizer, BertModel, BertConfig","metadata":{"execution":{"iopub.status.busy":"2024-03-09T19:14:03.332614Z","iopub.execute_input":"2024-03-09T19:14:03.332996Z","iopub.status.idle":"2024-03-09T19:14:13.945865Z","shell.execute_reply.started":"2024-03-09T19:14:03.332967Z","shell.execute_reply":"2024-03-09T19:14:13.944792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(\"device:\", device)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T19:14:13.948197Z","iopub.execute_input":"2024-03-09T19:14:13.949019Z","iopub.status.idle":"2024-03-09T19:14:13.955169Z","shell.execute_reply.started":"2024-03-09T19:14:13.948978Z","shell.execute_reply":"2024-03-09T19:14:13.953654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T19:14:13.956633Z","iopub.execute_input":"2024-03-09T19:14:13.956964Z","iopub.status.idle":"2024-03-09T19:14:13.971869Z","shell.execute_reply.started":"2024-03-09T19:14:13.956936Z","shell.execute_reply":"2024-03-09T19:14:13.970889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KFOLD = 5\n","metadata":{"execution":{"iopub.status.busy":"2024-03-09T19:14:13.973993Z","iopub.execute_input":"2024-03-09T19:14:13.974303Z","iopub.status.idle":"2024-03-09T19:14:13.982300Z","shell.execute_reply.started":"2024-03-09T19:14:13.974278Z","shell.execute_reply":"2024-03-09T19:14:13.980515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"\ntrain_df=pickle.load(open(\"/kaggle/input/nlp-disastertweets-eda-cleaning/train_cleaned_df.pkl\", \"rb\"))\ntest_df=pickle.load(open(\"/kaggle/input/nlp-disastertweets-eda-cleaning/test_cleaned_df.pkl\", \"rb\"))\n","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-09T19:14:15.085100Z","iopub.execute_input":"2024-03-09T19:14:15.085583Z","iopub.status.idle":"2024-03-09T19:14:15.131478Z","shell.execute_reply.started":"2024-03-09T19:14:15.085532Z","shell.execute_reply":"2024-03-09T19:14:15.130620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train data shape:\", train_df.shape)\nprint(\"test data shape:\", test_df.shape)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-09T19:14:15.133090Z","iopub.execute_input":"2024-03-09T19:14:15.134349Z","iopub.status.idle":"2024-03-09T19:14:15.141260Z","shell.execute_reply.started":"2024-03-09T19:14:15.134294Z","shell.execute_reply":"2024-03-09T19:14:15.140110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T19:14:15.143361Z","iopub.execute_input":"2024-03-09T19:14:15.143691Z","iopub.status.idle":"2024-03-09T19:14:15.175203Z","shell.execute_reply.started":"2024-03-09T19:14:15.143663Z","shell.execute_reply":"2024-03-09T19:14:15.173932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize the scentences\n\nHere I simply separate the words, since we have done cleaning","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\ndef tokenize(text):\n    output = tokenizer.encode_plus(\n        text,\n        None,\n        add_special_tokens=True,\n        max_length=30,\n        truncation=True,\n        padding=\"max_length\",\n        return_token_type_ids=True\n    )\n    return np.array([output[\"input_ids\"], output[\"token_type_ids\"], output[\"attention_mask\"]])","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:27:03.720004Z","iopub.execute_input":"2024-03-09T18:27:03.720364Z","iopub.status.idle":"2024-03-09T18:27:05.076773Z","shell.execute_reply.started":"2024-03-09T18:27:03.720338Z","shell.execute_reply":"2024-03-09T18:27:05.076054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\nX_origin = np.array(train_df['text'].progress_apply(lambda x : tokenize(x)))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:27:05.078133Z","iopub.execute_input":"2024-03-09T18:27:05.078466Z","iopub.status.idle":"2024-03-09T18:27:18.265306Z","shell.execute_reply.started":"2024-03-09T18:27:05.078423Z","shell.execute_reply":"2024-03-09T18:27:18.264381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.stack([X_origin], axis=1)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:29:05.290362Z","iopub.execute_input":"2024-03-09T18:29:05.291457Z","iopub.status.idle":"2024-03-09T18:29:05.298446Z","shell.execute_reply.started":"2024-03-09T18:29:05.291412Z","shell.execute_reply":"2024-03-09T18:29:05.297306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Expand dimension to match dataset class\nX_test = np.array(test_df['text'].progress_apply(lambda x : tokenize(x)))\nX_test = np.expand_dims(X_test, axis=1)\nX_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:28:43.965667Z","iopub.execute_input":"2024-03-09T18:28:43.966305Z","iopub.status.idle":"2024-03-09T18:28:46.234958Z","shell.execute_reply.started":"2024-03-09T18:28:43.966272Z","shell.execute_reply":"2024-03-09T18:28:46.234061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_df['target'].values","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:28:48.862932Z","iopub.execute_input":"2024-03-09T18:28:48.863327Z","iopub.status.idle":"2024-03-09T18:28:48.867997Z","shell.execute_reply.started":"2024-03-09T18:28:48.863300Z","shell.execute_reply":"2024-03-09T18:28:48.867007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create train/val data","metadata":{}},{"cell_type":"code","source":"\nkfold = KFold(n_splits=KFOLD, random_state=42, shuffle=True)\n\nfolds_idx_train = []\nfolds_idx_val = []\n\nfor fold_idx, (train_index, val_index) in enumerate(kfold.split(X)):\n    print(\"fold\", fold_idx, \"train:\", len(train_index), \"val:\", len(val_index))\n    folds_idx_train.append(train_index)\n    folds_idx_val.append(val_index)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:29:09.283461Z","iopub.execute_input":"2024-03-09T18:29:09.283811Z","iopub.status.idle":"2024-03-09T18:29:09.293657Z","shell.execute_reply.started":"2024-03-09T18:29:09.283785Z","shell.execute_reply":"2024-03-09T18:29:09.292646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create torch Dataset\nDataset can be used by Dataloader for input of the model","metadata":{}},{"cell_type":"code","source":"class NLPDs(Dataset):\n    def __init__(self, X, y=None):\n        self.X = X\n        self.y = y\n        print(\"NLPDs X.shape:\", X.shape)\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, index):\n        # randomly select augmentation if there is\n        X_item = self.X[index][random.randrange(0, self.X.shape[1])]\n        y_item = torch.tensor(self.y[index], dtype=torch.float) if self.y is not None else torch.tensor([])\n        return {\n            'X': torch.tensor(X_item, dtype=torch.long),\n            'y': y_item\n        }","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:40:32.513686Z","iopub.execute_input":"2024-03-09T18:40:32.514455Z","iopub.status.idle":"2024-03-09T18:40:32.521575Z","shell.execute_reply.started":"2024-03-09T18:40:32.514419Z","shell.execute_reply":"2024-03-09T18:40:32.520595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model selection\nSince this is a excercise for RNN, I will compare both pure RNN and LSTM if the perform differently.\nAfter testing both RNN and LSTM, it turns out LSTM has a much better performance than RNN. TO keep the Notebook concise, I have't keep the RNN code, but just show the best Model.","metadata":{}},{"cell_type":"code","source":"\nclass BertClassifier(nn.Module):\n\n    def __init__(\n        self, \n        fc_hidden_1_dim=128,\n        fc_hidden_2_dim=128,\n        dropout_p_1=0.9,\n        dropout_p_2=0.8,\n        dropout_p_3=0.7,\n        multi_dropout_sample_n = 8,\n    ):\n        super(BertClassifier, self).__init__()\n        self.multi_dropout_sample_n = multi_dropout_sample_n\n        \n        self.bert = transformers.BertModel.from_pretrained('bert-large-uncased')\n        \n        bert_features = 1024\n        \n        self.classifiers = nn.ModuleList()\n\n        for i in range(self.multi_dropout_sample_n):\n            self.classifiers.append(\n                nn.Sequential(\n                    nn.Dropout(p=dropout_p_1),\n                    nn.Linear(bert_features, fc_hidden_1_dim),\n                    nn.ReLU(),\n                    nn.Dropout(p=dropout_p_1),\n                    nn.Linear(fc_hidden_1_dim, fc_hidden_2_dim),\n                    nn.ReLU(),\n                    nn.Dropout(p=dropout_p_3),\n                    nn.Linear(fc_hidden_2_dim, 1),\n                    #nn.Sigmoid()\n                )\n            )\n    def forward(self, X):\n        #print(\"X:\", X.size())\n        input_ids, token_type_ids, attention_mask = X[:,0,:], X[:,1,:], X[:,2,:]\n        \n        out = self.bert(\n            input_ids, \n            attention_mask = attention_mask, \n            token_type_ids = token_type_ids, \n            return_dict=True).last_hidden_state\n        #https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/models/roberta/modeling_roberta.py#L567\n        out = out[:, 0, :]\n\n        # multi dropout sample\n        multi_dropout_out = []\n        for classifier in self.classifiers:\n            temp_out = classifier(out)\n            #print(\"out:\", out.size())\n            multi_dropout_out.append(temp_out)\n\n        out = torch.mean(torch.stack(multi_dropout_out), dim=0)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:29:27.457165Z","iopub.execute_input":"2024-03-09T18:29:27.458028Z","iopub.status.idle":"2024-03-09T18:29:27.468942Z","shell.execute_reply.started":"2024-03-09T18:29:27.457978Z","shell.execute_reply":"2024-03-09T18:29:27.468051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"markdown","source":"## Hyper parameters tuning\n\nHyper parameters tuning is very important, it decide the model training speed and perforamnce.\nIn my model, I turned the following parameers: learning rate, learning rate scheduler, optimizer, model size (LSTM hidden layer size, fully connected hidden layer size, etc.), regulation parameters like dropout rate, weight_decay, etc.\n\nIn order to keep the notebook concise, I didn't keep the code and result for all the different values of parameters, but put the final optimized values instead.","metadata":{}},{"cell_type":"code","source":"import copy\nfrom IPython.display import display, clear_output\nfrom torchmetrics.classification import BinaryAccuracy, BinaryF1Score\n\n@torch.inference_mode()\ndef validate(model, val_loader, show_progress):\n    if show_progress:\n        progress_bar = tqdm(total=len(val_loader), desc=f\"Validate\", leave=False)\n    \n    total_loss = 0\n    loss_function = nn.BCEWithLogitsLoss().to(device)\n    val_acc = BinaryAccuracy().to(device)\n    val_f1 = BinaryF1Score().to(device)\n    model.eval()\n    \n    for step, data in enumerate(val_loader):\n        \n        X, y = data[\"X\"].to(device, dtype=torch.long), data[\"y\"].to(device, dtype=torch.float)\n        \n        output = model(X)\n        output = torch.squeeze(output, dim=1)\n        loss = loss_function(output, y)\n        \n        output = F.sigmoid(output)\n        val_acc(output, y)    \n        val_f1(output, y)\n        \n        total_loss += loss.item()\n        \n        X.to(\"cpu\")\n        del X\n        y.to(\"cpu\")\n        del y\n\n        if show_progress:\n            progress_bar.set_postfix(\n                val_loss=(total_loss/(step+1)), \n                val_acc=val_acc.compute().item(), \n                val_f1=val_f1.compute().item(),\n            )\n            progress_bar.update()\n    if show_progress:    \n        progress_bar.close()\n    return total_loss/(len(val_loader)), val_acc.compute().item(), val_f1.compute().item()\n\n\ndef train(model, train_loader, fold_idx, epochs, loss_function, optimizer, scheduler, show_progress=False):\n    \n    fold_start_time = time.time()\n    \n    history = {\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"train_f1\": [],\n        \"val_loss\": [],\n        \"val_acc\": [],\n        \"val_f1\": []\n    }\n    best_val_f1 = 0\n    best_val_loss = np.inf\n    \n    for epoch in range(epochs):\n        epoch_start_time = time.time()\n        model.train()\n        if show_progress:\n            progress_bar = tqdm(total=len(train_loader), desc=f\"Fold {fold_idx} Epoch {epoch}\", leave=False)\n\n        total_loss = 0\n        train_acc = BinaryAccuracy().to(device)\n        train_f1 = BinaryF1Score().to(device)\n\n        for step, data in enumerate(train_loader):\n\n            optimizer.zero_grad()\n            \n            X, y = data[\"X\"].to(device, dtype=torch.long), data[\"y\"].to(device, dtype=torch.float)\n            output = torch.squeeze(model(X), dim=1)\n            loss = loss_function(output, y)\n            \n            output = F.sigmoid(output)\n            train_acc(output, y)\n            temp_f1 = train_f1(output, y)\n            \n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            \n            # release memory\n            X.to(\"cpu\")\n            del X\n            y.to(\"cpu\")\n            del y\n            \n            \n            current_train_loss = (total_loss/(step+1))\n            current_train_acc = train_acc.compute().item()\n            current_train_f1 = train_f1.compute().item()\n            \n            if show_progress:\n                progress_bar.set_postfix(\n                    loss=current_train_loss, \n                    acc=current_train_acc, \n                    f1=current_train_f1,\n                )\n                progress_bar.update()\n        \n        if show_progress:\n            progress_bar.close()\n            \n        scheduler.step()\n\n        val_loss, val_acc, val_f1 = validate(model, val_loader, show_progress)\n        \n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            \n            best_val_f1_model_wts = copy.deepcopy(model.state_dict())\n            \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_val_loss_model_wts = copy.deepcopy(model.state_dict())\n            \n        current_lr = round(scheduler.get_last_lr()[0], 8)\n        print(f\"Fold {fold_idx} Epoch {epoch}: \"\n              f\"{round(time.time()-epoch_start_time, 1)}s, \"\n              f\"loss={round(current_train_loss, 5)}, \"\n              f\"val_loss={round(val_loss, 5)}, \"\n              f\"acc={round(current_train_acc, 5)}, \"\n              f\"val_acc={round(val_acc, 5)}, \"\n              f\"f1={round(current_train_f1, 5)}, \"\n              f\"val_f1={round(val_f1, 5)}, \"\n              f\"lr={current_lr}\"\n             )\n        history[\"train_loss\"].append(current_train_loss)\n        history[\"train_acc\"].append(current_train_acc)\n        history[\"train_f1\"].append(current_train_f1)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        history[\"val_f1\"].append(val_f1)\n\n    torch.save(best_val_f1_model_wts, f\"fold_{fold_idx}_best_f1_model.bin\")\n    torch.save(best_val_loss_model_wts, f\"fold_{fold_idx}_best_loss_model.bin\")\n    \n    print(\"Fold\", fold_idx, \"best val F1 score:\", best_val_f1, \", best val loss:\", best_val_loss, \", duration:\", round(time.time()-fold_start_time, 1), \"s\")\n    return best_val_f1, best_val_loss, history\n\n\n# Move data in optimizer to cpu to release memory from gpu\ndef optimizer_to_cpu(optimizer):\n    cpu_device = \"cpu\"\n    for param in optimizer.state.values():\n        # Not sure there are any global tensors in the state dict\n        if isinstance(param, torch.Tensor):\n            param.data = param.data.to(cpu_device)\n            if param._grad is not None:\n                param._grad.data = param._grad.data.to(cpu_device)\n        elif isinstance(param, dict):\n            for subparam in param.values():\n                if isinstance(subparam, torch.Tensor):\n                    subparam.data = subparam.data.to(cpu_device)\n                    if subparam._grad is not None:\n                        subparam._grad.data = subparam._grad.data.to(cpu_device)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:29:34.352244Z","iopub.execute_input":"2024-03-09T18:29:34.352612Z","iopub.status.idle":"2024-03-09T18:29:34.379452Z","shell.execute_reply.started":"2024-03-09T18:29:34.352582Z","shell.execute_reply":"2024-03-09T18:29:34.378550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! rm -rf working/*","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:29:39.836697Z","iopub.execute_input":"2024-03-09T18:29:39.837574Z","iopub.status.idle":"2024-03-09T18:29:40.795604Z","shell.execute_reply.started":"2024-03-09T18:29:39.837532Z","shell.execute_reply":"2024-03-09T18:29:40.794490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 8\nLR = 1e-5\nMIN_LR = 1e-6\n\nFC_HIDDEN_1_DIM = 256\nFC_HIDDEN_2_DIM = 128\n\n\ntotal_f1 = 0\ntotal_loss = 0\nfor fold_idx in range(len(folds_idx_train)):\n    \n    train_idx = folds_idx_train[fold_idx]\n    val_idx = folds_idx_val[fold_idx]\n    X_train, y_train = X[train_idx], y[train_idx]\n    # for val we do not need augmentation\n    X_val, y_val = X[val_idx][:, 0:1], y[val_idx]\n    \n    train_ds = NLPDs(X_train, y_train)\n    val_ds = NLPDs(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=128, num_workers=2, shuffle=True, pin_memory=True, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=128, num_workers=2, shuffle=False, pin_memory=True, drop_last=False)\n\n    pos_weights = np.sum(1-y_train) / np.sum(y_train)\n    print(\"pos_weights:\", pos_weights)\n    \n    model = BertClassifier(\n        fc_hidden_1_dim=FC_HIDDEN_1_DIM, \n        fc_hidden_2_dim=FC_HIDDEN_2_DIM, \n        dropout_p_1=0.5,\n        dropout_p_2=0.5,\n        dropout_p_3=0.5,\n        multi_dropout_sample_n=1\n    )\n    model.to(device)\n    \n    loss_function = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weights)).to(device)\n    #loss_function = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LR, betas = (0.9,0.999),eps = 1.0*1e-8, weight_decay=1e-5)\n    scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=EPOCHS, eta_min=MIN_LR)\n\n\n    best_val_f1, best_val_loss, history = train(model, train_loader, fold_idx, EPOCHS, loss_function, optimizer, scheduler, show_progress=False)\n\n    total_f1 += best_val_f1\n    total_loss += best_val_loss\n    \n    # Relase gpu/cpu memory\n    model.to(\"cpu\")\n    optimizer_to_cpu(optimizer)\n    del model\n    del optimizer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    \nprint(\"Average val F1 score:\", round(total_f1/len(folds_idx_train), 5), \"Average val loss:\", round(total_loss/len(folds_idx_train), 5))","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:40:44.877863Z","iopub.execute_input":"2024-03-09T18:40:44.878695Z","iopub.status.idle":"2024-03-09T18:42:06.687255Z","shell.execute_reply.started":"2024-03-09T18:40:44.878656Z","shell.execute_reply":"2024-03-09T18:42:06.686074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relase gpu/cpu memory\ntry:\n    model.to(\"cpu\")\n    optimizer_to_cpu(optimizer)\n    del model\n    del optimizer\n    torch.cuda.empty_cache()\n    gc.collect()\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:31:09.695127Z","iopub.execute_input":"2024-03-09T18:31:09.696382Z","iopub.status.idle":"2024-03-09T18:31:09.705208Z","shell.execute_reply.started":"2024-03-09T18:31:09.696328Z","shell.execute_reply":"2024-03-09T18:31:09.704062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the metrics of the last fold","metadata":{}},{"cell_type":"code","source":"def plot_train_history(history, metric=\"acc\"):\n    train_metric = history[f\"train_{metric}\"]\n    val_metric = history[f\"val_{metric}\"]\n    plt.plot(history[f\"train_{metric}\"])\n    plt.plot(history[f\"val_{metric}\"])\n    plt.title(f\"{metric}\")\n    plt.ylabel(metric)\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n\nplot_train_history(history, \"f1\")\nplot_train_history(history, \"acc\")\nplot_train_history(history, \"loss\")","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:31:12.976573Z","iopub.execute_input":"2024-03-09T18:31:12.977209Z","iopub.status.idle":"2024-03-09T18:31:13.825164Z","shell.execute_reply.started":"2024-03-09T18:31:12.977178Z","shell.execute_reply":"2024-03-09T18:31:13.823263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference on test data","metadata":{}},{"cell_type":"markdown","source":"## Load models\n\nEnsemble models from each fold and average the results.","metadata":{}},{"cell_type":"code","source":"def load_weights(model, weights_path):\n    if os.path.isfile(weights_path):\n        model.load_state_dict(torch.load(weights_path))\n        print(\"Loaded weights from\", weights_path)\n        return True\n    else:\n        print(\"No previous weights available at\", weights_path)\n        return False","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:31:17.898792Z","iopub.execute_input":"2024-03-09T18:31:17.899625Z","iopub.status.idle":"2024-03-09T18:31:17.904696Z","shell.execute_reply.started":"2024-03-09T18:31:17.899593Z","shell.execute_reply":"2024-03-09T18:31:17.903623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nbest_f1_models = []\nfor fold_idx in range(len(folds_idx_train)):\n    model = BertClassifier(\n        fc_hidden_1_dim=FC_HIDDEN_1_DIM, \n        fc_hidden_2_dim=FC_HIDDEN_2_DIM, \n        dropout_p_1=0.95,\n        dropout_p_2=0.8,\n        dropout_p_3=0.6,\n        multi_dropout_sample_n=1\n    ).to(device)\n    loaded = load_weights(model, f\"fold_{fold_idx}_best_f1_model.bin\")\n    if loaded:\n        best_f1_models.append(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:31:18.874589Z","iopub.execute_input":"2024-03-09T18:31:18.874921Z","iopub.status.idle":"2024-03-09T18:31:26.636664Z","shell.execute_reply.started":"2024-03-09T18:31:18.874897Z","shell.execute_reply":"2024-03-09T18:31:26.635671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = NLPDs(X_test)\ntest_loader = DataLoader(test_ds, batch_size=32, num_workers=2, shuffle=False, pin_memory=True, drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:38:42.027421Z","iopub.execute_input":"2024-03-09T18:38:42.027790Z","iopub.status.idle":"2024-03-09T18:38:42.033408Z","shell.execute_reply.started":"2024-03-09T18:38:42.027764Z","shell.execute_reply":"2024-03-09T18:38:42.032375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.inference_mode()\ndef infer_by_one_model(model, test_loader):\n    progress_bar = tqdm(total=len(test_loader), desc=f\"Test\", leave=True)\n    \n    model.eval()\n    pred_list = []\n    \n    for step, data in enumerate(test_loader):\n        X = data[\"X\"].to(device, dtype=torch.long)\n        output = model(X)\n        output = torch.squeeze(output, dim=1)\n        pred_list.append(output.detach().cpu().numpy())\n        progress_bar.update()\n        \n        X.to(\"cpu\")\n        del X\n    \n    progress_bar.close()\n    return np.concatenate(pred_list)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:38:42.661570Z","iopub.execute_input":"2024-03-09T18:38:42.661927Z","iopub.status.idle":"2024-03-09T18:38:42.669063Z","shell.execute_reply.started":"2024-03-09T18:38:42.661899Z","shell.execute_reply":"2024-03-09T18:38:42.668209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate result by best val f1 model","metadata":{"execution":{"iopub.execute_input":"2024-03-05T21:04:42.313925Z","iopub.status.busy":"2024-03-05T21:04:42.313527Z","iopub.status.idle":"2024-03-05T21:04:42.319607Z","shell.execute_reply":"2024-03-05T21:04:42.318581Z","shell.execute_reply.started":"2024-03-05T21:04:42.313890Z"}}},{"cell_type":"code","source":"\nmodels_results = []\nfor model in best_f1_models:\n    models_results.append(infer_by_one_model(model, test_loader))\n\nprob = np.sum(np.stack(models_results), axis=0) / len(best_f1_models)\n\ntest_df[\"target_prob\"] = prob\ntest_df[\"target\"] = test_df[\"target_prob\"].apply(lambda x: 1 if x>=0.5 else 0)\nsum(test_df[\"target\"]==1)\n\nsmpl_sub = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nsub = pd.merge(smpl_sub[[\"id\"]], test_df[[\"id\",\"target\"]], on=\"id\", how=\"left\")\n\nsub.to_csv(\"best_f1_submission.csv\", index=False)\n\nsub.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:38:43.427761Z","iopub.execute_input":"2024-03-09T18:38:43.428594Z","iopub.status.idle":"2024-03-09T18:38:54.203550Z","shell.execute_reply.started":"2024-03-09T18:38:43.428559Z","shell.execute_reply":"2024-03-09T18:38:54.202430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in best_f1_models:\n    model.to(\"cpu\")\n    del model\n\ndel best_f1_models\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:39:04.046782Z","iopub.execute_input":"2024-03-09T18:39:04.047188Z","iopub.status.idle":"2024-03-09T18:39:26.225473Z","shell.execute_reply.started":"2024-03-09T18:39:04.047136Z","shell.execute_reply":"2024-03-09T18:39:26.224352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate result by best val loss model","metadata":{"execution":{"iopub.execute_input":"2024-03-05T21:04:51.706535Z","iopub.status.busy":"2024-03-05T21:04:51.706169Z","iopub.status.idle":"2024-03-05T21:04:51.730626Z","shell.execute_reply":"2024-03-05T21:04:51.729646Z","shell.execute_reply.started":"2024-03-05T21:04:51.706507Z"}}},{"cell_type":"code","source":"best_loss_models = []\nfor fold_idx in range(len(folds_idx_train)):\n    model = BertClassifier(\n        fc_hidden_1_dim=FC_HIDDEN_1_DIM, \n        fc_hidden_2_dim=FC_HIDDEN_2_DIM, \n        dropout_p_1=0.95,\n        dropout_p_2=0.8,\n        dropout_p_3=0.6,\n        multi_dropout_sample_n=1\n    ).to(device)\n    loaded = load_weights(model, f\"fold_{fold_idx}_best_loss_model.bin\")\n    if loaded:\n        best_loss_models.append(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:39:26.227378Z","iopub.execute_input":"2024-03-09T18:39:26.227992Z","iopub.status.idle":"2024-03-09T18:39:34.957644Z","shell.execute_reply.started":"2024-03-09T18:39:26.227956Z","shell.execute_reply":"2024-03-09T18:39:34.956758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodels_results = []\nfor model in best_loss_models:\n    models_results.append(infer_by_one_model(model, test_loader))\n\nprob = np.sum(np.stack(models_results), axis=0) / len(best_loss_models)\n\ntest_df[\"target_prob\"] = prob\ntest_df[\"target\"] = test_df[\"target_prob\"].apply(lambda x: 1 if x>=0.5 else 0)\nsum(test_df[\"target\"]==1)\n\nsmpl_sub = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nsub = pd.merge(smpl_sub[[\"id\"]], test_df[[\"id\",\"target\"]], on=\"id\", how=\"left\")\n\nsub.to_csv(\"best_loss_submission.csv\", index=False)\n\nsub.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:39:39.917695Z","iopub.execute_input":"2024-03-09T18:39:39.918068Z","iopub.status.idle":"2024-03-09T18:39:50.782217Z","shell.execute_reply.started":"2024-03-09T18:39:39.918038Z","shell.execute_reply":"2024-03-09T18:39:50.781086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in best_loss_models:\n    model.to(\"cpu\")\n    del model\n\ndel best_loss_models\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T18:39:55.204322Z","iopub.execute_input":"2024-03-09T18:39:55.205082Z","iopub.status.idle":"2024-03-09T18:39:56.531142Z","shell.execute_reply.started":"2024-03-09T18:39:55.205047Z","shell.execute_reply":"2024-03-09T18:39:56.530248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}