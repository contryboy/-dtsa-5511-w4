{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":166302698,"sourceType":"kernelVersion"},{"sourceId":166308478,"sourceType":"kernelVersion"},{"sourceId":166312251,"sourceType":"kernelVersion"}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary\n\nThe target of the project is to create a supervised model with sequencial data deep learning model, to predict if a tweet indicates a real disaster. This is useful for some organizations to actively monitoring tweets for deteting disaster in near real time. ","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport string\nimport pickle\nimport time\nimport gc\n\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchmetrics\n\n# Import tqdm for progress bar\nfrom tqdm.notebook import tqdm\n\nimport transformers\nfrom transformers import RobertaTokenizer, RobertaConfig, RobertaModel","metadata":{"execution":{"iopub.status.busy":"2024-03-10T12:50:16.922418Z","iopub.execute_input":"2024-03-10T12:50:16.923424Z","iopub.status.idle":"2024-03-10T12:50:16.929931Z","shell.execute_reply.started":"2024-03-10T12:50:16.923377Z","shell.execute_reply":"2024-03-10T12:50:16.928924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(\"device:\", device)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T12:50:17.794157Z","iopub.execute_input":"2024-03-10T12:50:17.795211Z","iopub.status.idle":"2024-03-10T12:50:17.827319Z","shell.execute_reply.started":"2024-03-10T12:50:17.795174Z","shell.execute_reply":"2024-03-10T12:50:17.826344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T12:50:18.569060Z","iopub.execute_input":"2024-03-10T12:50:18.569922Z","iopub.status.idle":"2024-03-10T12:50:18.578425Z","shell.execute_reply.started":"2024-03-10T12:50:18.569889Z","shell.execute_reply":"2024-03-10T12:50:18.577669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KFOLD = 5\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T12:50:19.372871Z","iopub.execute_input":"2024-03-10T12:50:19.373764Z","iopub.status.idle":"2024-03-10T12:50:19.377825Z","shell.execute_reply.started":"2024-03-10T12:50:19.373718Z","shell.execute_reply":"2024-03-10T12:50:19.376973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"\ntrain_df=pickle.load(open(\"/kaggle/input/nlp-disastertweets-1-eda-cleaning/train_cleaned_df.pkl\", \"rb\"))\ntest_df=pickle.load(open(\"/kaggle/input/nlp-disastertweets-1-eda-cleaning/test_cleaned_df.pkl\", \"rb\"))\n\ntransback_data = pickle.load(open(\"/kaggle/input/nlp-disastertweets-2-translation/translated_text_list.pkl\", \"rb\"))\n\neda_sr_data = pickle.load(open(\"/kaggle/input/nlp-disastertweets-3-easydataaugmentation/eda_sr_text_list.pkl\", \"rb\"))\neda_ri_data = pickle.load(open(\"/kaggle/input/nlp-disastertweets-3-easydataaugmentation/eda_ri_text_list.pkl\", \"rb\"))\neda_rs_data = pickle.load(open(\"/kaggle/input/nlp-disastertweets-3-easydataaugmentation/eda_rs_text_list.pkl\", \"rb\"))\neda_rd_data = pickle.load(open(\"/kaggle/input/nlp-disastertweets-3-easydataaugmentation/eda_rd_text_list.pkl\", \"rb\"))\ntrain_df.sample(n=10).head()","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-10T13:13:54.817242Z","iopub.execute_input":"2024-03-10T13:13:54.817949Z","iopub.status.idle":"2024-03-10T13:13:54.972458Z","shell.execute_reply.started":"2024-03-10T13:13:54.817913Z","shell.execute_reply":"2024-03-10T13:13:54.971544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"trans_back_text\"] = np.array(transback_data)\n\ntrain_df[\"eda_sr_0\"] = np.array(eda_sr_data)[:, 0]\ntrain_df[\"eda_sr_1\"] = np.array(eda_sr_data)[:, 1]\n\ntrain_df[\"eda_ri_0\"] = np.array(eda_ri_data)[:, 0]\ntrain_df[\"eda_ri_1\"] = np.array(eda_ri_data)[:, 1]\n\ntrain_df[\"eda_rs_0\"] = np.array(eda_rs_data)[:, 0]\ntrain_df[\"eda_rs_1\"] = np.array(eda_rs_data)[:, 1]\n\ntrain_df[\"eda_rd_0\"] = np.array(eda_rd_data)[:, 0]\ntrain_df[\"eda_rd_1\"] = np.array(eda_rd_data)[:, 1]\n\ntrain_df.sample(5).head()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:13:55.601606Z","iopub.execute_input":"2024-03-10T13:13:55.602285Z","iopub.status.idle":"2024-03-10T13:13:55.814001Z","shell.execute_reply.started":"2024-03-10T13:13:55.602256Z","shell.execute_reply":"2024-03-10T13:13:55.813069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train data shape:\", train_df.shape)\nprint(\"test data shape:\", test_df.shape)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-10T13:13:56.396114Z","iopub.execute_input":"2024-03-10T13:13:56.396871Z","iopub.status.idle":"2024-03-10T13:13:56.401498Z","shell.execute_reply.started":"2024-03-10T13:13:56.396839Z","shell.execute_reply":"2024-03-10T13:13:56.400605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize the scentences\n\nHere I simply separate the words, since we have done cleaning","metadata":{}},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-large\")\n\ndef tokenize(text):\n    output = tokenizer.encode_plus(\n        text,\n        None,\n        add_special_tokens=True,\n        max_length=30,\n        truncation=True,\n        padding=\"max_length\",\n        return_token_type_ids=True\n    )\n    return np.array([output[\"input_ids\"], output[\"token_type_ids\"], output[\"attention_mask\"]])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:13:57.333129Z","iopub.execute_input":"2024-03-10T13:13:57.333909Z","iopub.status.idle":"2024-03-10T13:13:57.627129Z","shell.execute_reply.started":"2024-03-10T13:13:57.333877Z","shell.execute_reply":"2024-03-10T13:13:57.626178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\nX_origin = np.array(train_df['text'].progress_apply(lambda x : tokenize(x)))\nX_trans_back = np.array(train_df['trans_back_text'].progress_apply(lambda x : tokenize(x)))\nX_eda_sr_0 = np.array(train_df['eda_sr_0'].progress_apply(lambda x : tokenize(x)))\nX_eda_sr_1 = np.array(train_df['eda_sr_1'].progress_apply(lambda x : tokenize(x)))\n\nX_eda_ri_0 = np.array(train_df['eda_ri_0'].progress_apply(lambda x : tokenize(x)))\nX_eda_ri_1 = np.array(train_df['eda_ri_1'].progress_apply(lambda x : tokenize(x)))\n\nX_eda_rs_0 = np.array(train_df['eda_rs_0'].progress_apply(lambda x : tokenize(x)))\nX_eda_rs_1 = np.array(train_df['eda_rs_1'].progress_apply(lambda x : tokenize(x)))\n\nX_eda_rd_0 = np.array(train_df['eda_rd_0'].progress_apply(lambda x : tokenize(x)))\nX_eda_rd_1 = np.array(train_df['eda_rd_1'].progress_apply(lambda x : tokenize(x)))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:13:57.936176Z","iopub.execute_input":"2024-03-10T13:13:57.936580Z","iopub.status.idle":"2024-03-10T13:14:18.677828Z","shell.execute_reply.started":"2024-03-10T13:13:57.936550Z","shell.execute_reply":"2024-03-10T13:14:18.676877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X = np.stack([X_origin, X_eda_sr_0, X_eda_ri_0, X_eda_rs_0, X_eda_rd_0], axis=1)\nX = np.stack([X_origin, X_trans_back], axis=1)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:14:18.679657Z","iopub.execute_input":"2024-03-10T13:14:18.680354Z","iopub.status.idle":"2024-03-10T13:14:18.688541Z","shell.execute_reply.started":"2024-03-10T13:14:18.680319Z","shell.execute_reply":"2024-03-10T13:14:18.687673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Expand dimension to match dataset class\nX_test = np.array(test_df['text'].progress_apply(lambda x : tokenize(x)))\nX_test = np.expand_dims(X_test, axis=1)\nX_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:14:18.689795Z","iopub.execute_input":"2024-03-10T13:14:18.690436Z","iopub.status.idle":"2024-03-10T13:14:19.892353Z","shell.execute_reply.started":"2024-03-10T13:14:18.690403Z","shell.execute_reply":"2024-03-10T13:14:19.891509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_df['target'].values","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:14:19.894101Z","iopub.execute_input":"2024-03-10T13:14:19.894380Z","iopub.status.idle":"2024-03-10T13:14:19.898834Z","shell.execute_reply.started":"2024-03-10T13:14:19.894357Z","shell.execute_reply":"2024-03-10T13:14:19.898002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create train/val data","metadata":{}},{"cell_type":"code","source":"\n\nkfold = KFold(n_splits=KFOLD, random_state=42, shuffle=True)\n\nfolds_idx_train = []\nfolds_idx_val = []\n\nfor fold_idx, (train_index, val_index) in enumerate(kfold.split(X)):\n    print(\"fold\", fold_idx, \"train:\", len(train_index), \"val:\", len(val_index))\n    folds_idx_train.append(train_index)\n    folds_idx_val.append(val_index)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:32.657854Z","iopub.execute_input":"2024-03-10T13:03:32.658211Z","iopub.status.idle":"2024-03-10T13:03:32.667515Z","shell.execute_reply.started":"2024-03-10T13:03:32.658183Z","shell.execute_reply":"2024-03-10T13:03:32.666572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create torch Dataset\nDataset can be used by Dataloader for input of the model","metadata":{}},{"cell_type":"code","source":"class NLPDs(Dataset):\n    def __init__(self, X, y=None):\n        self.X = X\n        self.y = y\n        print(\"NLPDs X.shape:\", X.shape)\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, index):\n        # randomly select augmentation if there is\n        X_item = self.X[index][random.randrange(0, self.X.shape[1])]\n        y_item = torch.tensor(self.y[index], dtype=torch.float) if self.y is not None else torch.tensor([])\n        return {\n            'X': torch.tensor(X_item, dtype=torch.long),\n            'y': y_item\n        }","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:33.990565Z","iopub.execute_input":"2024-03-10T13:03:33.990912Z","iopub.status.idle":"2024-03-10T13:03:33.998517Z","shell.execute_reply.started":"2024-03-10T13:03:33.990885Z","shell.execute_reply":"2024-03-10T13:03:33.997581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model selection\nSince this is a excercise for RNN, I will compare both pure RNN and LSTM if the perform differently.\nAfter testing both RNN and LSTM, it turns out LSTM has a much better performance than RNN. TO keep the Notebook concise, I have't keep the RNN code, but just show the best Model.","metadata":{}},{"cell_type":"code","source":"\nclass BertClassifier(nn.Module):\n\n    def __init__(\n        self, \n        fc_hidden_1_dim=128,\n        fc_hidden_2_dim=128,\n        dropout_p_1=0.5,\n        dropout_p_2=0.5,\n        dropout_p_3=0.5,\n        multi_dropout_sample_n = 8,\n    ):\n        super(BertClassifier, self).__init__()\n        self.multi_dropout_sample_n = multi_dropout_sample_n\n        \n        self.bert = RobertaModel.from_pretrained(\"FacebookAI/roberta-large\")\n        \n        bert_features = 1024\n        \n        #print(\"dropout_p:\", dropout_p)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout_p_1),\n            nn.Linear(bert_features, fc_hidden_1_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_p_1),\n            nn.Linear(fc_hidden_1_dim, fc_hidden_2_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_p_3),\n            nn.Linear(fc_hidden_2_dim, 1),\n        )\n        \n    def forward(self, X):\n        #print(\"X:\", X.size())\n        input_ids, token_type_ids, attention_mask = X[:,0,:], X[:,1,:], X[:,2,:]\n        \n        out = self.bert(\n            input_ids, \n            attention_mask = attention_mask, \n            token_type_ids = token_type_ids, \n            return_dict=True).last_hidden_state\n\n        #https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/models/roberta/modeling_roberta.py#L567\n        out = out[:, 0, :]\n        \n        # multi dropout sample\n        multi_dropout_out = []\n        for classifier in range(self.multi_dropout_sample_n):\n            temp_out = self.classifier(out)\n            #print(\"out:\", out.size())\n            multi_dropout_out.append(temp_out)\n\n        return multi_dropout_out\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:35.470197Z","iopub.execute_input":"2024-03-10T13:03:35.470567Z","iopub.status.idle":"2024-03-10T13:03:35.480794Z","shell.execute_reply.started":"2024-03-10T13:03:35.470539Z","shell.execute_reply":"2024-03-10T13:03:35.479876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyper parameters tuning\n\nHyper parameters tuning is very important, it decide the model training speed and perforamnce.\nIn my model, I turned the following parameers: learning rate, learning rate scheduler, optimizer, model size (LSTM hidden layer size, fully connected hidden layer size, etc.), regulation parameters like dropout rate, weight_decay, etc.\n\nIn order to keep the notebook concise, I didn't keep the code and result for all the different values of parameters, but put the final optimized values instead.","metadata":{}},{"cell_type":"code","source":"import copy\nfrom IPython.display import display, clear_output\nfrom torchmetrics.classification import BinaryAccuracy, BinaryF1Score\nfrom torchvision.ops import sigmoid_focal_loss \n\nFOCAL_ALPHA = 0.6\nFOCAL_GAMMA = 1\n@torch.inference_mode()\ndef validate(model, val_loader, show_progress):\n    if show_progress:\n        progress_bar = tqdm(total=len(val_loader), desc=f\"Validate\", leave=False)\n    \n    total_loss = 0\n    loss_function = nn.BCEWithLogitsLoss().to(device)\n    val_acc = BinaryAccuracy(threshold=0.5).to(device)\n    val_f1_list = [BinaryF1Score(threshold=0.45+i/100).to(device) for i in range(10)]\n    model.eval()\n    \n    for step, data in enumerate(val_loader):\n        \n        X, y = data[\"X\"].to(device, dtype=torch.long), data[\"y\"].to(device, dtype=torch.float)\n\n        # for inference mode, there is no dropout, all multi samples are the same, we just get the first one\n        output = model(X)[0]\n\n        output = torch.squeeze(output, dim=1)\n        \n        loss = loss_function(output, y)\n        \n        output = F.sigmoid(output)\n        val_acc(output, y)\n        \n        for val_f1 in val_f1_list:\n            val_f1(output, y)\n        \n        total_loss += loss.item()\n        \n        X.to(\"cpu\")\n        del X\n        y.to(\"cpu\")\n        del y\n\n        if show_progress:\n            progress_bar.set_postfix(\n                val_loss=(total_loss/(step+1)), \n                val_acc=val_acc.compute().item(), \n            )\n            progress_bar.update()\n    if show_progress:    \n        progress_bar.close()\n\n    # get the best f1 score threshold\n    best_f1_score = 0\n    best_f1_threshold = 0.5\n    for val_f1 in val_f1_list:\n        temp_score = val_f1.compute().item()\n        if temp_score > best_f1_score:\n            best_f1_score = temp_score\n            best_f1_threshold = val_f1.threshold\n    print(\"best f1 threshold:\", best_f1_threshold, \"ref f1 score (0.5):\", val_f1_list[5].compute().item())\n    return total_loss/(len(val_loader)), val_acc.compute().item(), best_f1_score, best_f1_threshold\n\n\ndef train(model, train_loader, fold_idx, epochs, loss_function, optimizer, scheduler, show_progress=False):\n    \n    fold_start_time = time.time()\n    \n    history = {\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"train_f1\": [],\n        \"val_loss\": [],\n        \"val_acc\": [],\n        \"val_f1\": []\n    }\n    best_val_f1 = 0\n    best_val_f1_threshold = 0.5\n    best_val_loss = np.inf\n    best_val_acc = 0\n    \n    for epoch in range(epochs):\n        epoch_start_time = time.time()\n        model.train()\n        if show_progress:\n            progress_bar = tqdm(total=len(train_loader), desc=f\"Fold {fold_idx} Epoch {epoch}\", leave=False)\n\n        total_loss = 0\n        train_acc = BinaryAccuracy().to(device)\n        train_f1 = BinaryF1Score().to(device)\n\n        for step, data in enumerate(train_loader):\n\n            optimizer.zero_grad()\n            \n            X, y = data[\"X\"].to(device, dtype=torch.long), data[\"y\"].to(device, dtype=torch.float)\n\n            # The output is now a list of dropout sample\n            output = model(X)\n            losses = []\n            output_sigmoids = []\n            for out in output:\n                out = torch.squeeze(out, dim=1)\n\n                losses.append(loss_function(out, y))\n                sigmoid_value = F.sigmoid(out.detach())\n                \n                output_sigmoids.append(sigmoid_value)\n\n            loss = torch.mean(torch.stack(losses))\n\n            output = torch.mean(torch.stack(output_sigmoids), dim=0)\n            \n            train_acc(output, y)\n            temp_f1 = train_f1(output, y)\n            \n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            \n            # release memory\n            X.to(\"cpu\")\n            del X\n            y.to(\"cpu\")\n            del y\n            \n            \n            current_train_loss = (total_loss/(step+1))\n            current_train_acc = train_acc.compute().item()\n            current_train_f1 = train_f1.compute().item()\n            \n            if show_progress:\n                progress_bar.set_postfix(\n                    loss=current_train_loss, \n                    acc=current_train_acc, \n                    f1=current_train_f1,\n                )\n                progress_bar.update()\n        \n        if show_progress:\n            progress_bar.close()\n            \n        scheduler.step()\n\n        val_loss, val_acc, val_f1, f1_threshold = validate(model, val_loader, show_progress)\n        \n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_val_f1_threshold = f1_threshold\n            best_val_f1_model_wts = copy.deepcopy(model.state_dict())\n            \n            \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            \n        current_lr = round(scheduler.get_last_lr()[0], 8)\n        print(f\"Fold {fold_idx} Epoch {epoch}: \"\n              f\"{round(time.time()-epoch_start_time, 1)}s, \"\n              f\"loss={round(current_train_loss, 5)}, \"\n              f\"val_loss={round(val_loss, 5)}, \"\n              f\"acc={round(current_train_acc, 5)}, \"\n              f\"val_acc={round(val_acc, 5)}, \"\n              f\"f1={round(current_train_f1, 5)}, \"\n              f\"val_f1={round(val_f1, 5)}, \"\n              f\"lr={current_lr}\"\n             )\n        history[\"train_loss\"].append(current_train_loss)\n        history[\"train_acc\"].append(current_train_acc)\n        history[\"train_f1\"].append(current_train_f1)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        history[\"val_f1\"].append(val_f1)\n\n    torch.save(best_val_f1_model_wts, f\"fold_{fold_idx}_best_f1_model.bin\")\n    \n    print(\"Fold\", fold_idx, \"best val F1 score:\", best_val_f1, \n          \", best val loss:\", best_val_loss, \n          \", best val acc:\", best_val_acc, \n          \", duration:\", round(time.time()-fold_start_time, 1), \"s\")\n    return best_val_f1, best_val_f1_threshold, best_val_loss, best_val_acc, history\n\n\n# Move data in optimizer to cpu to release memory from gpu\ndef optimizer_to_cpu(optimizer):\n    cpu_device = \"cpu\"\n    for param in optimizer.state.values():\n        # Not sure there are any global tensors in the state dict\n        if isinstance(param, torch.Tensor):\n            param.data = param.data.to(cpu_device)\n            if param._grad is not None:\n                param._grad.data = param._grad.data.to(cpu_device)\n        elif isinstance(param, dict):\n            for subparam in param.values():\n                if isinstance(subparam, torch.Tensor):\n                    subparam.data = subparam.data.to(cpu_device)\n                    if subparam._grad is not None:\n                        subparam._grad.data = subparam._grad.data.to(cpu_device)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:36.896427Z","iopub.execute_input":"2024-03-10T13:03:36.896805Z","iopub.status.idle":"2024-03-10T13:03:36.929348Z","shell.execute_reply.started":"2024-03-10T13:03:36.896777Z","shell.execute_reply":"2024-03-10T13:03:36.928320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#! rm -rf output/*","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:38.048712Z","iopub.execute_input":"2024-03-10T13:03:38.049060Z","iopub.status.idle":"2024-03-10T13:03:38.053213Z","shell.execute_reply.started":"2024-03-10T13:03:38.049027Z","shell.execute_reply":"2024-03-10T13:03:38.052229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FC_HIDDEN_1_DIM = 256\nFC_HIDDEN_2_DIM = 128","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:39.238804Z","iopub.execute_input":"2024-03-10T13:03:39.239159Z","iopub.status.idle":"2024-03-10T13:03:39.243441Z","shell.execute_reply.started":"2024-03-10T13:03:39.239132Z","shell.execute_reply":"2024-03-10T13:03:39.242488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 8\nLR = 1e-5\nMIN_LR = 1e-6\n\n\ntotal_f1 = 0\ntotal_acc = 0\ntotal_loss = 0\nf1_hresholds = []\nfor fold_idx in range(len(folds_idx_train)):\n#if True:\n#    fold_idx = 3\n    train_idx = folds_idx_train[fold_idx]\n    val_idx = folds_idx_val[fold_idx]\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx][:, 0:1], y[val_idx]\n    \n    train_ds = NLPDs(X_train, y_train)\n    val_ds = NLPDs(X_val, y_val)\n\n    train_loader = DataLoader(train_ds, batch_size=128, num_workers=2, shuffle=True, pin_memory=True, drop_last=False)\n    val_loader = DataLoader(val_ds, batch_size=128, num_workers=2, shuffle=False, pin_memory=True, drop_last=False)\n\n    pos_weights = np.sum(1-y_train) / np.sum(y_train)\n    print(\"pos_weights:\", pos_weights)\n    \n    model = BertClassifier(\n        fc_hidden_1_dim=FC_HIDDEN_1_DIM, \n        fc_hidden_2_dim=FC_HIDDEN_2_DIM, \n        dropout_p_1=0.5,\n        dropout_p_2=0.5,\n        dropout_p_3=0.5,\n        multi_dropout_sample_n=16\n    )\n    model.to(device)\n    \n    loss_function = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weights)).to(device)\n    #loss_function = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LR, betas = (0.9,0.999),eps = 1.0*1e-8, weight_decay=1e-4)\n    scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=EPOCHS, eta_min=MIN_LR)\n\n\n    best_val_f1, best_val_f1_threshold, best_val_loss, best_val_acc, history = train(model, train_loader, fold_idx, EPOCHS, loss_function, optimizer, scheduler, show_progress=False)\n    \n    total_f1 += best_val_f1\n    f1_hresholds.append(best_val_f1_threshold)\n    total_acc += best_val_acc\n    total_loss += best_val_loss\n    \n    # Relase gpu/cpu memory\n    model.to(\"cpu\")\n    optimizer_to_cpu(optimizer)\n    del model\n    del optimizer\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    #break\n    \nprint(\"Average val F1 score:\", round(total_f1/len(folds_idx_train), 5), \n      \"Average val Acc:\", round(total_acc/len(folds_idx_train), 5), \n      \"Average val loss:\", round(total_loss/len(folds_idx_train), 5))","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:03:41.637844Z","iopub.execute_input":"2024-03-10T13:03:41.638196Z","iopub.status.idle":"2024-03-10T13:13:47.974003Z","shell.execute_reply.started":"2024-03-10T13:03:41.638168Z","shell.execute_reply":"2024-03-10T13:13:47.972318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mannually free up memroy if above code is interrupted\ntry:\n    model.to(\"cpu\")\n    optimizer_to_cpu(optimizer)\n    del model\n    del optimizer\n    torch.cuda.empty_cache()\n    gc.collect()\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:43.299413Z","iopub.execute_input":"2024-03-10T13:02:43.299826Z","iopub.status.idle":"2024-03-10T13:02:48.546038Z","shell.execute_reply.started":"2024-03-10T13:02:43.299782Z","shell.execute_reply":"2024-03-10T13:02:48.545119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the metrics of the last fold","metadata":{}},{"cell_type":"code","source":"def plot_train_history(history, metric=\"acc\"):\n    train_metric = history[f\"train_{metric}\"]\n    val_metric = history[f\"val_{metric}\"]\n    plt.plot(history[f\"train_{metric}\"])\n    plt.plot(history[f\"val_{metric}\"])\n    plt.title(f\"{metric}\")\n    plt.ylabel(metric)\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n\nplot_train_history(history, \"f1\")\nplot_train_history(history, \"acc\")\nplot_train_history(history, \"loss\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference on test data","metadata":{}},{"cell_type":"markdown","source":"## Load test data","metadata":{}},{"cell_type":"code","source":"test_ds = NLPDs(X_test)\ntest_loader = DataLoader(test_ds, batch_size=32, num_workers=2, shuffle=False, pin_memory=True, drop_last=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions\n\nEnsemble models from each fold and average the results.","metadata":{}},{"cell_type":"code","source":"def infer_by_best_metric_models(metric_name=\"f1\", threshold=0.5):\n    print(\"Threshold:\", threshold)\n    best_models = []\n    for fold_idx in range(len(folds_idx_train)):\n        model = BertClassifier(\n            fc_hidden_1_dim=FC_HIDDEN_1_DIM, \n            fc_hidden_2_dim=FC_HIDDEN_2_DIM, \n            dropout_p_1=0.5,\n            dropout_p_2=0.5,\n            dropout_p_3=0.5,\n            multi_dropout_sample_n=8\n        ).to(device)\n        loaded = load_weights(model, f\"fold_{fold_idx}_best_{metric_name}_model.bin\")\n        if loaded:\n            best_models.append(model)\n            \n    models_results = []\n    for model in best_models:\n        models_results.append(infer_by_one_model(model, test_loader))\n    \n    prob = np.sum(np.stack(models_results), axis=0) / len(best_models)\n    \n    test_df[\"target_prob\"] = prob\n    test_df[\"target\"] = test_df[\"target_prob\"].apply(lambda x: 1 if x>=threshold else 0)\n    \n    smpl_sub = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n    sub = pd.merge(smpl_sub[[\"id\"]], test_df[[\"id\",\"target\"]], on=\"id\", how=\"left\")\n    sub.to_csv(f\"best_{metric_name}_submission.csv\", index=False)\n    \n    # clean up gpu/cpu memory\n    for model in best_models:\n        model.to(\"cpu\")\n        del model\n    del best_models\n    torch.cuda.empty_cache()\n    gc.collect()\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_weights(model, weights_path):\n    if os.path.isfile(weights_path):\n        model.load_state_dict(torch.load(weights_path))\n        print(\"Loaded weights from\", weights_path)\n        return True\n    else:\n        print(\"No previous weights available at\", weights_path)\n        return False\n\n@torch.inference_mode()\ndef infer_by_one_model(model, test_loader):\n    progress_bar = tqdm(total=len(test_loader), desc=f\"Test\", leave=True)\n    \n    model.eval()\n    pred_list = []\n    \n    for step, data in enumerate(test_loader):\n        X = data[\"X\"].to(device, dtype=torch.long)\n        output = model(X)[0]\n        output = torch.squeeze(output, dim=1)\n        pred_list.append(output.detach().cpu().numpy())\n        progress_bar.update()\n\n        X.to(\"cpu\")\n    \n    progress_bar.close()\n    return np.concatenate(pred_list)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate result by best val f1 model","metadata":{}},{"cell_type":"code","source":"f1_avg_threshold = sum(f1_hresholds)/len(f1_hresholds)\ninfer_by_best_metric_models(\"f1\", f1_avg_threshold)","metadata":{},"execution_count":null,"outputs":[]}]}